from curl_cffi import requests # æ›æˆé€™å€‹
from bs4 import BeautifulSoup
import json
import time
import random

def get_ptt_mentor_data_v2(pages=10):
    base_url = "https://www.ptt.cc"
    current_url = "/bbs/Boy-Girl/index.html"
    
    # å»ºç«‹ä¸€å€‹æ¨¡æ“¬ Chrome 120 çš„ Session
    session = requests.Session()
    
    mentor_data = []

    for i in range(pages):
        print(f"ğŸ“¡ æ­£åœ¨æ¨¡æ“¬ç€è¦½å™¨åˆ†æ PTT ç¬¬ {i+1} é ...")
        try:
            # ä½¿ç”¨ impersonate="chrome120" ç¹é TLS æª¢æ¸¬
            res = session.get(base_url + current_url, impersonate="chrome120", cookies={'over18': '1'}, timeout=10)
            
            if res.status_code != 200:
                print(f"âŒ é é¢è«‹æ±‚å¤±æ•—: {res.status_code}")
                break
                
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = soup.select('div.r-ent')
            
            for art in articles:
                title_tag = art.select_one('div.title a')
                # åªæŠ“ã€ŒRe:ã€é–‹é ­çš„æ–‡ç« ï¼Œé€™äº›æ‰æ˜¯å°å¸«çš„å›è¦†å…§å®¹
                if title_tag and "Re:" in title_tag.text:
                    url = base_url + title_tag['href']
                    
                    # é€²å…¥æ–‡ç« å…§å®¹
                    art_res = session.get(url, impersonate="chrome120", cookies={'over18': '1'}, timeout=10)
                    art_soup = BeautifulSoup(art_res.text, 'html.parser')
                    
                    # æŠ“å–å…§æ–‡
                    main_content = art_soup.select_one('#main-content')
                    if not main_content: continue
                    
                    # æ¸…æ´—å…§å®¹ï¼šç§»é™¤æ‰æ¨æ–‡èˆ‡è½‰ä¿¡ç«™è³‡è¨Š
                    full_text = main_content.text
                    clean_text = full_text.split('â€» ç™¼ä¿¡ç«™:')[0]
                    
                    if len(clean_text) > 150: # å¤ é•·æ‰æœ‰æ•™è‚²æ„ç¾©
                        print(f"âœ… æŠ“å–åˆ°å°å¸«æ–‡ï¼š{title_tag.text[:15]}...")
                        mentor_data.append({
                            "instruction": "[äººæ ¼:æ„Ÿæƒ…å°å¸«] ä½ æ˜¯ä¸€ä½æ´å¯ŸåŠ›æ•éŠ³çš„æ„Ÿæƒ…å°ˆå®¶ï¼Œè«‹åˆ†æä¸¦å›è¦†ä»¥ä¸‹å›°å¢ƒã€‚",
                            "input": f"å€‹æ¡ˆæƒ…å¢ƒï¼š{title_tag.text.replace('Re: ', '')}",
                            "output": clean_text.strip()
                        })
                    
                    # éš¨æ©Ÿå»¶é²ï¼Œé˜²æ­¢è¢« PTT æ¨™è¨˜ IP
                    time.sleep(random.uniform(1, 3))

            # æŠ“å–ä¸Šä¸€é é€£çµ
            btns = soup.select('div.btn-group-paging a')
            current_url = btns[1]['href'] # index 1 æ˜¯ã€Œä¸Šé ã€

        except Exception as e:
            print(f"ğŸ’¥ ç™¼ç”Ÿé€£ç·šç•°å¸¸: {e}")
            time.sleep(5)
            continue

    # å„²å­˜
    with open("ptt_mentor_data_real.jsonl", "w", encoding="utf-8") as f:
        for entry in mentor_data:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    print(f"ğŸ‰ ä»»å‹™å®Œæˆï¼å…±ç²å¾— {len(mentor_data)} ç­†é«˜å“è³ªæ„Ÿæƒ…å°å¸«èªæ–™ã€‚")

if __name__ == "__main__":
    get_ptt_mentor_data_v2(15)