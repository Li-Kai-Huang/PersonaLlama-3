import requests
from bs4 import BeautifulSoup
import time
import json
import re

def get_ptt_real_simp_data(target_pages=20):
    base_url = "https://www.ptt.cc"
    current_url = "/bbs/Boy-Girl/index.html"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    cookies = {'over18': '1'}
    
    simp_dataset = []
    # åªè¦æ˜¯æ±‚åŠ©ã€å¿ƒæƒ…ã€åˆ†äº«ï¼Œé€šé€šä¸æ”¾é
    target_tags = ["[æ±‚åŠ©]", "[å¿ƒæƒ…]", "[åˆ†äº«]"]
    # å…§æ–‡é—œéµå­—ï¼šåªè¦å…§æ–‡å‡ºç¾é€™äº›è©ï¼Œé€™ç¯‡å°±å¾ˆå¯èƒ½æ˜¯ã€Œèˆ”ç‹—/å‘å¾®ã€ä¸»é¡Œ
    body_keywords = ["å·¥å…·äºº", "å‘å¾®", "æš—æˆ€", "è¨Šæ¯", "å·²è®€", "ä¸å›", "å®µå¤œ", "æ¥é€"]

    for i in range(target_pages):
        print(f"æ­£åœ¨æƒæçœ‹æ¿ç¬¬ {i+1} é ...")
        try:
            res = requests.get(base_url + current_url, headers=headers, cookies=cookies, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = soup.select('div.r-ent')
            
            for art in articles:
                title_tag = art.select_one('div.title a')
                if not title_tag: continue
                title = title_tag.text
                
                # åªè¦æ¨™é¡Œç¬¦åˆæ¨™ç±¤ï¼Œå°±é»é€²å»
                if any(tag in title for tag in target_tags):
                    art_url = base_url + title_tag['href']
                    art_res = requests.get(art_url, headers=headers, cookies=cookies, timeout=10)
                    art_soup = BeautifulSoup(art_res.text, 'html.parser')
                    content = art_soup.select_one('#main-content').text
                    
                    # æª¢æŸ¥å…§æ–‡æ˜¯å¦åŒ…å«å‘å¾®é—œéµå­—
                    if any(word in content for word in body_keywords):
                        print(f"æ‰¾åˆ°æ½›åœ¨ç›®æ¨™: {title}")
                        # æŠ“å–ã€Œã€å…§çš„æ–‡å­—ï¼Œæˆ–è€…æ˜¯ A: B é€™ç¨®æ ¼å¼
                        # ä½¿ç”¨æ­£è¦è¡¨é”å¼æŠ“å–æ‹¬è™Ÿå…§å®¹
                        quotes = re.findall(r'[ã€Œã€](.*?)[ã€ã€]', content)
                        for q in quotes:
                            if 5 < len(q) < 100: # éæ¿¾å¤ªçŸ­æˆ–å¤ªé•·çš„äº‚ç¢¼
                                simp_dataset.append({
                                    "instruction": f"åœ¨æƒ…å¢ƒã€{title}ã€ä¸­ï¼Œæœ‰ä¸€æ®µå‘å¾®çš„å°è©±ï¼š",
                                    "input": "",
                                    "output": q.strip()
                                })
                    time.sleep(0.5)

            prev_btn = soup.select('div.btn-group-paging a')[1]
            current_url = prev_btn['href']
            
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")
            continue

    with open("real_ptt_data_v2.jsonl", "w", encoding="utf-8") as f:
        for entry in simp_dataset:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    print(f"ğŸ‰ æŠ“å–çµæŸï¼å…±ç²å¾— {len(simp_dataset)} ç­†èªæ–™ã€‚")

if __name__ == "__main__":
    get_ptt_real_simp_data(30) # æƒå¤šä¸€é»ï¼Œæƒ 30 é 