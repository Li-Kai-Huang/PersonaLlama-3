from unsloth import FastLanguageModel

# è¼‰å…¥ä½ å‰›ç·´å¥½çš„æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "mixed_personality_v4_final", # æŒ‡å‘ä½  save_pretrained çš„è·¯å¾‘
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

def ask_llama(user_input, explicit_tag=None):
    # å¦‚æœæ²’çµ¦æ¨™ç±¤ï¼Œå°±è®“ Llama-3 è‡ªå·±çŒœ
    tag = f"[{explicit_tag}] " if explicit_tag else ""
    prompt = f"### Instruction:\n{user_input}\n\n### Response:\n"
    inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens = 512, temperature = 0.7)
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split("### Response:\n")[-1]

# --- å ±å‘Šç”¨çš„æ¸¬è©¦æ¡ˆä¾‹ ---
print("ğŸ§ª æ¡ˆä¾‹ 1ï¼šNLP å°ˆæ¥­å•é¡Œï¼ˆä¸åŠ æ¨™ç±¤ï¼‰")
print(ask_llama("è«‹è§£é‡‹ç‚ºä»€éº¼ Transformer æ¯” RNN é©åˆè™•ç†é•·æ–‡æœ¬ï¼Ÿ"))

print("\nğŸ§ª æ¡ˆä¾‹ 2ï¼šæ„Ÿæƒ…æ±‚åŠ©ï¼ˆä¸åŠ æ¨™ç±¤ï¼‰")
print(ask_llama("æˆ‘çš„æ›–æ˜§å°è±¡çªç„¶ä¸å›æˆ‘äº†ï¼Œæˆ‘è©²å‚³ä»€éº¼çµ¦å¥¹ï¼Ÿ"))