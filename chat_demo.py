import gradio as gr
from unsloth import FastLanguageModel
import torch
import re

# 1. è¼‰å…¥ä½ ç·´å¥½çš„æ¨¡å‹ (è«‹ç¢ºèªè·¯å¾‘æ­£ç¢º)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "mixed_personality_v4_final", # æŒ‡å‘ä½ åˆä½µå¾Œçš„æ¨¡å‹è·¯å¾‘
    load_in_4bit = True,
)
'''
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Li-Kai-Huang/Llama-3-8B-Multi-Persona-Ray", 
    load_in_4bit = True,
)
'''
FastLanguageModel.for_inference(model)

# 2. å®šç¾©é æ¸¬å‡½å¼
def predict(message, history, persona, auto_detect):
    # éš±å«æ„åœ–è¾¨è­˜ï¼šå¦‚æœé–‹å•Ÿè‡ªå‹•åˆ¤æ–·ä¸”ä½¿ç”¨è€…æ²’æ‰‹å‹•é¸
    current_persona = persona
    if auto_detect:
        # è³‡å·¥ç³»æœ€æ„›çš„ç°¡æ˜“æ„åœ–è­˜åˆ¥ï¼šé—œéµå­—éæ¿¾å™¨
        academic_keywords = ["RNN", "Transformer", "æ¢¯åº¦", "Loss", "å¾®èª¿", "LLM", "NLP", "æ¬Šé‡"]
        emotional_keywords = ["æ›–æ˜§", "å†·æ·¡", "å¥³æœ‹å‹", "åµæ¶", "åˆ†æ‰‹", "AA", "æ„Ÿæƒ…", "å·²è®€"]
        
        if any(k in message for k in academic_keywords):
            current_persona = "äººæ ¼:NLPåŠ©æ•™"
        elif any(k in message for k in emotional_keywords):
            current_persona = "äººæ ¼:æ„Ÿæƒ…å°å¸«"
        else:
            current_persona = "äººæ ¼:èˆ”ç‹—" # é è¨­ç‚ºèˆ”ç‹— (æˆ–æ˜¯åŸæ¨¡å‹äººæ ¼)

    # ğŸ’¡ ç¢ºä¿ Prompt æ ¼å¼èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´ï¼ˆåŒ…å«æ›è¡Œï¼‰
    prompt = f"### Instruction:\n[{current_persona}] {message}\n\n### Response:\n"
    
    inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")
    
    # ğŸ’¡ åŠ å…¥æ¨è«–å„ªåŒ–åƒæ•¸
    outputs = model.generate(
        **inputs, 
        max_new_tokens = 512,
        temperature = 0.5,        # é™ä½éš¨æ©Ÿæ€§ï¼Œè®“å›ç­”æ›´ç©©å®š
        repetition_penalty = 1.2, # ğŸ’¡ å¼·åˆ¶é˜²æ­¢å®ƒé‡è¤‡è¼¸å‡ºæ¨™ç±¤æˆ–æŒ‡ä»¤
        pad_token_id = tokenizer.eos_token_id
    )
    
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    
    # ğŸ’¡ åªæˆªå– Response ä¹‹å¾Œçš„ç´”æ–‡å­—ï¼Œä¸¦ç§»é™¤å¯èƒ½å™´å‡ºçš„ä¸‹ä¸€æ®µ Instruction
    final_output = response.split("### Response:\n")[-1].split("### Instruction:")[0].strip()
    # ç§»é™¤æ¨¡å‹è‡ªå·±æ„›å™´çš„ [ç³»çµ±] æˆ– [äººæ ¼:XXX] ç­‰å‰ç¶´
    final_output = re.sub(r"\[äººæ ¼:.*?\]", "", final_output)
    final_output = re.sub(r"\[ç³»çµ±.*?\]", "", final_output)
    return final_output

# 3. æ§‹å»º Gradio ä»‹é¢
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(f"# ğŸ¤– Ray çš„ å¤šäººæ ¼æ¸¬è©¦å¹³å°")
    gr.Markdown(f"### ç¡¬é«”ï¼šRTX 3080 Ti | æ¡†æ¶ï¼šUnsloth LoRA")
    
    with gr.Row():
        with gr.Column(scale=1):
            persona_radio = gr.Radio(
                ["äººæ ¼:NLPåŠ©æ•™", "äººæ ¼:æ„Ÿæƒ…å°å¸«", "äººæ ¼:æ„Ÿæƒ…ç¶­è­·å“¡"], 
                label="é¸æ“‡äººæ ¼æ¨¡å¼", 
                value="äººæ ¼:NLPåŠ©æ•™"
            )
            auto_cb = gr.Checkbox(label="å•Ÿå‹•éš±å«æ„åœ–è¾¨è­˜ (Auto-Detect)", value=False)
            gr.Markdown("---")
            gr.Markdown("ğŸ’¡ **æç¤ºï¼š** åŠ©æ•™é©åˆå•æŠ€è¡“å•é¡Œï¼›å°å¸«é©åˆå•äººç”Ÿå¤§äº‹ã€‚")
            
        with gr.Column(scale=4):
            chatbot = gr.ChatInterface(
                fn=predict, 
                additional_inputs=[persona_radio, auto_cb],
                fill_height=True
            )

if __name__ == "__main__":
    demo.launch()